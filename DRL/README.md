### 表格型
#### Q-learning


#### Sarsa







### 表格型近似
Why:
- 当状态空间扩展到任意大时，我们将没有足够的大的内存来存储大表格，并且精确的填充耗费的时间也会是一个问题
- 我们需要在没有见过的状态下做出合理的决定，这是很常见的
- 要想将强化学习系统应用到人工智能和大型工程应用中，系统必须有能力进行泛化。

How:
为了能够实现这个目标，现存的大量的有监督学习函数逼近的任何方法都可以使用，只要将每次更新时涉及的二元组`s->g`作为训练样本来使用就可以了。

#### 神经网络
逼近价值函数的参数化方法(最适合的有监督学习方法)：
通过某种学习算法来调整某个逼近函数的参数，来近似在整个状态空间上定义的价值函数。

非线性方法包括用反向传播和各种SGD训练的人工神经网络，称为深度强化学习


#### DQN
DQN算法由以下几部分构成：
- Q learning主框架
- 记忆库（用于重复学习）
- 神经网络计算Q值
- 暂时冻结q_target参数（切断相关性）

$L(w)=\frac{1}{2} \sum_{i=1}^{N}\left(f\left(x_{i}, w\right)-y_{i}\right)^{2}$

$\mathbf{w}_{t+1}=\mathbf{w}_{t}+\alpha\left[R_{t+1}+\gamma \max _{a} \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)$
### Tool
#### gym




